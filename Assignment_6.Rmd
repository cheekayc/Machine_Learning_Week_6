---
title: "Assignment 6"
author: "Chee Kay Cheong"
date: "2023-02-21"
output: github_document
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(rpart) # construct CaRT
library(caret) # construct CaRT
library(rpart.plot) # makes cleaner looking tree plots
library(pROC) # generate ROC
library(e1071) # SVM 
library(NHANES)
```

# Part 1 

## Load and clean dataset, Data Partitioning

Restrict the `NHANES` data to the list of 11 variables below. 

Variables: "Age", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100".

```{r}
# Load dataset
data("NHANES")

# Select variables, clean dataset, remove missing values, set reference group for outcome
nhanes = NHANES %>% 
  janitor::clean_names() %>% 
  select(age, race1, education, hh_income, weight, height, pulse, diabetes, bmi, phys_active, smoke100) %>% 
  drop_na() 

# Set 'No' as reference level for `diabetes`
nhanes$diabetes = relevel(nhanes$diabetes, ref = "No")

# Check proportion of outcome 
nhanes %>% 
  select(diabetes) %>% 
  group_by(diabetes) %>% 
  count()
# Yes = 659, No = 5697, Not balanced...
```

Partition the data into training and testing using a 70/30 split.
```{r}
set.seed(123)

training.index = 
  nhanes$diabetes %>% 
  createDataPartition(p = 0.7, list = F)

training = nhanes[training.index, ]
testing = nhanes[-training.index, ]

# I want to see if I have enough outcomes in my training dataset and if I should do "up" or "down" sample.
training %>% 
  select(diabetes) %>% 
  group_by(diabetes) %>% 
  count()
```

# Part 2 & 3

## Construct 3 prediction models, choose hyperparameters, and compare performance 

### Classification Tree

```{r}
set.seed(123)

# Create 10-fold cross-validation and use up-sampling because of imbalance in outcome
train.control = trainControl(method = "cv", number = 10, sampling = "up")

# Create sequence of cp parameters to try 
grid = expand.grid(cp = seq(0.001, 0.5, by = 0.01))

# Train model
class_tree = train(diabetes ~ ., data = training, method = "rpart", trControl = train.control, tuneGrid = grid)

# Find best tune cp
class_tree
class_tree$bestTune

# Obtain metrics of accuracy from training
confusionMatrix(class_tree)
```
The cp value = 0.491 has the highest accuracy (0.8961806). Average accuracy of this model is 0.8962. Note that none of the 'Yes' diabetes were correctly predicted as 'Yes' diabetes.

### Support Vector Classifier

```{r}
set.seed(123)

# Use same control settings as created in code chunk above
# Incorporate different values for cost (C)
tune_grid = expand.grid(C = seq(0.001, 2, length = 50))

svm_model = train(diabetes ~ ., data = training, method = "svmLinear", trControl = train.control, preProcess = c("center", "scale"), tuneGrid = tune_grid)

# Visualize accuracy versus values of C
plot(svm_model)

# Obtain metrics of accuracy from training
confusionMatrix(svm_model)

#See information about final model
svm_model$results
svm_model$finalModel
```
The best tuned parameter (C) = 1.8776. It has 4697 support vectors, with an average training error of 0.2434 and an average accuracy of 0.7169.

### Logistic Regression

```{r}
set.seed(123)

logreg = train(diabetes ~ ., data = training, method = "glm", trControl = train.control, preProcess = c("center", "scale"))

logreg$results

confusionMatrix(logreg)
```
Average accuracy = 0.7306.

# Part 4

### Select "optimal" model

I have performed Accuracy Test to compare performance of all three models. Although the Classification Tree model gives us the highest accuracy among all three models, its sensitivity is 0. None of the 'Yes' diabetes were correctly predicted as 'Yes' diabetes, so I decided not to use this model for Diabetes prediction.

Between the SVM and Logistic Regression models, I decided to choose the **SVM model** although the Logistic Regression model has a higher accuracy than the SVM model. However, according to the confusion matrix, more percentage of 'Yes' diabetes were correctly predicted as 'Yes' diabetes in the SVM model (8.3% compared to 7.9%). 

### Apply model to test set and calculate evaluation metrics

I decided to use **Confusion Matrix** as the evaluation metrics for the test set because I don't know how to interpret an ROC curve.
```{r}
# Make predictions in test set
svm.pred.test = predict(svm_model, testing)

# Get evaluation metrics from test set
confusionMatrix(svm.pred.test, testing$diabetes, positive = "Yes")
```

Based on the Confusion Matrix, the SVM model has an accuracy of 0.70 (95% CI = 0.675, 0.717), with a sensitivity of 0.84 and a specificity of 0.68. The proportion of false positive 
is quite high because the positive predictive value is very low (0.23). 

# Part 5

### Limitations/considerations of the model 

List and describe at least two limitations/considerations of the model generated by this analysis. Limitations can be analytical or they can be considerations that need to be made regarding how the model would be applied in practice.

1. Sensitivity to choice of kernel and hyperparameters: SVM models rely heavily on the choice of kernel and its associated hyperparameters. Different kernel functions and hyperparameters may lead to very different results. In this assignment, I do not know the range of Cost (i.e. what is the minimum and maximum value of this hyperparameter?)

2. Difficulty with large datasets: SVM models can become computationally expensive when working with large datasets. Training time can become prohibitively long when the number of observations and/or features is very large. This is especially true if the dataset is not well-balanced or if there is a high degree of overlap between classes. This happened to me when I train the SVM model using the training set. My computer has been running for 10 minutes to complete that one line of code. Additionally, the large memory requirements of SVM models can also be a concern when working with very large datasets.


