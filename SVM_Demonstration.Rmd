---
title: "SVM Demonstration"
author: "JAS"
output: github_document
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(caret)
library(e1071) # SVM 
library(kernlab) #???
library(pROC) # generate ROC
```

# Demonstration of Support Vector Classifiers

Data Citation: We are using a dataset containing features related to heart disease. There are 13 features and the outcome variable is a binary, classification variable indicating the presence of heart disease.

***

### Load data and perform minor cleaning, check and recode missings etc.
1. How to load a flat text file
2. How to assign column names when none are provided
3. How to check variable types across the dataframe
4. How to recode missing indicators, change variable types and explore variable distributions

```{r data_prep_svc}
# Read in dataset
heart.data = read.csv("./Data/processed.cleveland.data", header = FALSE)

# Create informative variable names
var.names = c("age", "sex", "pain_type", "resting_sysbp", "chol", "fast_blsugar_gt120", "rest_ecg", "max_hr", "exerc_angina", "ST_depression", "ST_slope", "vessels_colorflu", "defect", "heart_disease_present")

# Assign variable names to each column
colnames(heart.data) = var.names

# Check variable types
str(heart.data)

# Find any observation with a "?" and convert it to NA
heart.data[heart.data == "?"] = NA

# Change variable types (must make the outcome a factor!)
heart.data = 
  heart.data %>% 
  mutate(
    defect = as.numeric(factor(defect)),
    vessels_colorflu = as.numeric(factor(vessels_colorflu)),
    outcome = as_factor(ifelse(heart_disease_present == 0, 0, 1)), # if heart_disease_present = 0, outcome = 0; else, outcome = 1.
    outcome = fct_recode(outcome, 'HDNotPresent' = '0', 'HDPresent' = '1')) %>% 
  select(-heart_disease_present)

summary(heart.data)

#Remove the missings
heart.data.nomiss = na.omit(heart.data)

# For the `heart.data.nomiss` dataset, set 'No Heart Disease' as Reference Level
heart.data.nomiss$outcome = relevel(heart.data.nomiss$outcome, ref = "HDNotPresent")
```

### Partition data into training and testing

```{r datapart_svc}
set.seed(123)

train.indices = createDataPartition(y = heart.data.nomiss$outcome, p = 0.7, list = FALSE)
training = heart.data.nomiss[train.indices, ]
testing = heart.data.nomiss[-train.indices, ]
```


### Train Support Vector Classifier (or Support Vector Machine with Linear Kernel) in `Caret`

`Caret` doesn't automatically tune the hyperparameter *C* (*C* controls how much misclassification the SVM will accept). We need to specify values to try. The smaller the value of C, the less misclassification the SVM will accept.

The default measures is accuracy. We can do predicted probabilities, AUROC, etc.
```{r svc}
modelLookup("svmLinear")

set.seed(123)

#Set control settings: 10-fold cross-validation. If we want predicted probabilities, we need to set "classProbs = True".
train_control = trainControl(method = "cv", number = 10, classProbs = T)

#Train model. Note we are scaling data because SVM is based on distance.
svm.caret = train(outcome ~ ., data = training, method = "svmLinear", trControl = train_control, preProcess = c("center", "scale"))

# Show results
svm.caret
```

The results show that there are 208 samples in the training model, 13 predictors, 2 classes of outcome. We have preprocessed (center and scale) the data and resampled using cross-validation. The average accuracy is 0.8170996. **Note that we did not tune the parameter (C)**, so it says the tuning parameter 'C' was held constant at a value of 1.

### Tune the hyperparameter

```{r}
set.seed(123)

# Incorporate different values for cost (C)
tune_grid = expand.grid(C = seq(0.001, 2, length = 30))

svm.caret.2 = train(outcome ~ ., data = training, method = "svmLinear", trControl = train_control, preProcess = c("center", "scale"), tuneGrid = tune_grid)

# Visualize accuracy versus values of C
plot(svm.caret.2)

# Obtain metrics of accuracy from training
confusionMatrix(svm.caret.2)

#See information about final model
svm.caret.2$finalModel
```
We can see from the result that using different values of C (other than 1) gives us a higher overall accuracy (0.8413). R automatically picks the best tuned value of C as the final model. The best tuned C is 0.2078. It has 86 support vectors, with an average training error of 0.1442. It also tells us that we have included a probability model. 

### Apply model to test set and get evaluation metrics

Option 1: Confusion Matrix
```{r}
#Make predictions in testset
svm.pred.test = predict(svm.caret.2, testing)

#Get evaluation metrics from test set
confusionMatrix(svm.pred.test, testing$outcome, positive = "HDPresent")
```

Option 2: ROC 
```{r}
# First created predicted probablities using data in the test set
pred.prob = predict(svm.caret.2, testing, type = "prob")

# Then conduct analysis using `roc` function
analysis = roc(response = testing$outcome, predictor = pred.prob[ ,2]) # Response = actual observed, predictor = predicted probabilities

# Plot to see the ROC curve
plot(1-analysis$specificities, analysis$sensitivities, type = "l",
  ylab = "Sensitivity", xlab = "1-Specificity", col = "black", lwd = 2,
  main = "ROC Curve for Heart Disease Classification")
abline(a = 0, b = 1)
```

### Notes about Variable Importance

Variable Importance for the SVC For SVM classification models, most packages do not have built-in variable importance. The default behavior in `caret` for SVM is to compute the area under the ROC curve. FROM DOCUMENTATION FOR CARET For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. More detail is 

https://topepo.github.io/caret/variable-importance.html

```{r}
varImp(svm.caret.2)
```

